{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Prompt using Automatic Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from AutomaticPromptEngineer import APE, configs\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of goals, depressed, anxious categories to sample from \n",
    "\n",
    "goals = ['불안 완화', '관계 개선', '생산성 향상', '우울 완화', '자존감 향상', \n",
    "            '자신감 향상', '스트레스 완화', '행복 증대']\n",
    "depressed = ['정상', '경미한', '중간', '약간 심한', '심한']\n",
    "anxious = ['정상', '경미한', '중간', '약간 심한', '심한']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대분류</th>\n",
       "      <th>중분류</th>\n",
       "      <th>소분류</th>\n",
       "      <th>세부분류</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>감정</td>\n",
       "      <td>감정조절이상</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>저 사실, 약간 중2병 같은 걸 증상을 보이고 있습니다.\\n그래서 대학교를 가면서도...</td>\n",
       "      <td>사우님이 중2병적인 행동과 태도를 가지고 있다고 느껴져 스트레스를 받으시는 상황입니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>배경</td>\n",
       "      <td>직장</td>\n",
       "      <td>스트레스</td>\n",
       "      <td>NaN</td>\n",
       "      <td>회사에서 일이 너무 바빠서 스트레스 받아요.\\n회사에서 일이 너무 많아서 스트레스 ...</td>\n",
       "      <td>사우님이 회사에서 일이 너무 많아서 지쳐서 스트레스를 받고 있으시다는데요. 하루종일...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>감정</td>\n",
       "      <td>불안감</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>남들에게 이상해보인다는 생각 때문에 항상 불안하고 스트레스 받아요. 눈치보지 말라고...</td>\n",
       "      <td>사우님 남들에게 이상하게 보일까봐 불안하고 스트레스 받으시네요. 눈치보지 말라고 해...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>감정</td>\n",
       "      <td>심란</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>요즘 회사에서 일이 많아지면서 나 혼자 다 이뤄내야 한다는 압박감이 들어요. 이전에...</td>\n",
       "      <td>사우님은 회사에서 일이 많아져서 혼자 다 처리해야 하는 압박감 때문에 불안감을 느끼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>감정</td>\n",
       "      <td>걱정</td>\n",
       "      <td>불면</td>\n",
       "      <td>NaN</td>\n",
       "      <td>저는 요즘 일이 많아지면서 체력적으로도, 정신적으로도 지치게 됐어요. 마감기한이 얼...</td>\n",
       "      <td>사우님 요즘 일이 많아지면서 체력적으로도, 정신적으로도 지친 상태이고 일 할 때마다...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  대분류     중분류   소분류 세부분류                                              input  \\\n",
       "0  감정  감정조절이상   NaN  NaN  저 사실, 약간 중2병 같은 걸 증상을 보이고 있습니다.\\n그래서 대학교를 가면서도...   \n",
       "1  배경      직장  스트레스  NaN  회사에서 일이 너무 바빠서 스트레스 받아요.\\n회사에서 일이 너무 많아서 스트레스 ...   \n",
       "2  감정     불안감   NaN  NaN  남들에게 이상해보인다는 생각 때문에 항상 불안하고 스트레스 받아요. 눈치보지 말라고...   \n",
       "3  감정      심란   NaN  NaN  요즘 회사에서 일이 많아지면서 나 혼자 다 이뤄내야 한다는 압박감이 들어요. 이전에...   \n",
       "4  감정      걱정    불면  NaN  저는 요즘 일이 많아지면서 체력적으로도, 정신적으로도 지치게 됐어요. 마감기한이 얼...   \n",
       "\n",
       "                                              output  \n",
       "0  사우님이 중2병적인 행동과 태도를 가지고 있다고 느껴져 스트레스를 받으시는 상황입니...  \n",
       "1  사우님이 회사에서 일이 너무 많아서 지쳐서 스트레스를 받고 있으시다는데요. 하루종일...  \n",
       "2  사우님 남들에게 이상하게 보일까봐 불안하고 스트레스 받으시네요. 눈치보지 말라고 해...  \n",
       "3  사우님은 회사에서 일이 많아져서 혼자 다 처리해야 하는 압박감 때문에 불안감을 느끼...  \n",
       "4  사우님 요즘 일이 많아지면서 체력적으로도, 정신적으로도 지친 상태이고 일 할 때마다...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download dataset \n",
    "root = '/Users/jungkyungyoon/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer'\n",
    "data_path = os.path.join(root, 'data')\n",
    "file_path = os.path.join(data_path, 'mindbut_classification_merged_file.csv')\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Instruction \n",
    "inst_path = os.path.join(data_path, 'inst.txt')\n",
    "with open(inst_path, 'r') as file:\n",
    "    # read file\n",
    "    content = file.read()\n",
    "\n",
    "# split by '-----'\n",
    "content_list = content.split('-----')\n",
    "instructions = [item.strip() for item in content_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Get configs path\n",
    "configs_path = os.path.join(root, 'configs', 'model.yaml')\n",
    "\n",
    "# Read YAML file\n",
    "with open(configs_path, 'r') as file:\n",
    "    configs = yaml.safe_load(file)\n",
    "\n",
    "config_generation = configs['generation']\n",
    "config_eval = configs['evaluation']\n",
    "config_score = configs['score']\n",
    "\n",
    "print(config_generation['num_queries_per_subsample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('APE')\n",
    "\n",
    "from generate import generate_prompt, eval_prompt, score_prompt\n",
    "from evaluate import evaluate_prompts, EvaluationResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'GPT', 'gpt_config': {'model': 'gpt-3.5-turbo-1106', 'temperature': 0.9, 'max_tokens': 500, 'top_p': 0.9}}\n"
     ]
    }
   ],
   "source": [
    "print(config_generation['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT] Generating 150 completions, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/APE/llm.py:82\u001b[0m, in \u001b[0;36mGPT.__complete\u001b[0;34m(self, prompt, n)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     83\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig, messages\u001b[39m=\u001b[39;49mmessages)\n\u001b[1;32m     84\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    599\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    600\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    601\u001b[0m             {\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    620\u001b[0m             },\n\u001b[1;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    622\u001b[0m         ),\n\u001b[1;32m    623\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    627\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1060\u001b[0m opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1061\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1062\u001b[0m )\n\u001b[0;32m-> 1063\u001b[0m \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    843\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    844\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    845\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    846\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    847\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    848\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:873\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    874\u001b[0m         options,\n\u001b[1;32m    875\u001b[0m         cast_to,\n\u001b[1;32m    876\u001b[0m         retries,\n\u001b[1;32m    877\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    878\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    879\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    880\u001b[0m     )\n\u001b[1;32m    882\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:933\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    931\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 933\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    934\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    935\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    936\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    937\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    938\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    939\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:873\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    874\u001b[0m         options,\n\u001b[1;32m    875\u001b[0m         cast_to,\n\u001b[1;32m    876\u001b[0m         retries,\n\u001b[1;32m    877\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    878\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    879\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    880\u001b[0m     )\n\u001b[1;32m    882\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:933\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    931\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 933\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    934\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    935\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    936\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    937\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    938\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    939\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jungkyungyoon/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/demo.ipynb 셀 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jungkyungyoon/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/demo.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Generate Prompt \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jungkyungyoon/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/demo.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m prompts \u001b[39m=\u001b[39m generate_prompt(instructions, config_generation)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jungkyungyoon/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/demo.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m prompts[\u001b[39m3\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/APE/generate.py:28\u001b[0m, in \u001b[0;36mgenerate_prompt\u001b[0;34m(instructions, config)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# Instantiate the LLM\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39mmodel_from_config(config[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m], disable_tqdm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m prompts \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mcomplete(\n\u001b[1;32m     29\u001b[0m     queries, n\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mnum_queries_per_subsample\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m prompts\n",
      "File \u001b[0;32m~/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/APE/llm.py:62\u001b[0m, in \u001b[0;36mGPT.complete\u001b[0;34m(self, prompt, n)\u001b[0m\n\u001b[1;32m     60\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m tqdm(prompt, disable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_tqdm):\n\u001b[0;32m---> 62\u001b[0m     temp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__complete(p, n)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m temp:\n\u001b[1;32m     64\u001b[0m         res\u001b[39m.\u001b[39mappend(i)\n",
      "File \u001b[0;32m~/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/APE/llm.py:87\u001b[0m, in \u001b[0;36mGPT.__complete\u001b[0;34m(self, prompt, n)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m     86\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRetrying...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m5\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m [response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][i][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m]))]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate Prompt \n",
    "prompts = generate_prompt(instructions, config_generation)\n",
    "prompts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'APE' has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jungkyungyoon/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/demo.ipynb 셀 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jungkyungyoon/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/demo.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Generate outputs of counselor given the prompt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jungkyungyoon/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/demo.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m outputs \u001b[39m=\u001b[39m APE\u001b[39m.\u001b[39;49mgenerate\u001b[39m.\u001b[39meval_prompt(prompts, goals, depressed, anxious, data, config_eval)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jungkyungyoon/Desktop/Academics/YBIGTA/23-2/Conference/Model/AutomaticPromptEngineer/demo.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m outputs\u001b[39m.\u001b[39mhead()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'APE' has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "# Generate outputs of counselor given the prompt\n",
    "outputs = eval_prompt(prompts, goals, depressed, anxious, data, config_eval)\n",
    "outputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the scores\n",
    "scores = score_prompt(outputs, config_score)\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best prompts \n",
    "SCORES = evaluate_prompts(scores)\n",
    "best_prompts = EvaluationResult.sorted(SCORES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
